from contextlib import asynccontextmanager
import datetime
import os
from pathlib import Path
from loguru import logger
from apis.xhs_pc_apis import XHS_Apis
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
from dotenv import load_dotenv
from xhs_utils.database import connect_to_mongo, close_mongo_connection, get_database


# æ˜ç¡®æŒ‡å®š .env æ–‡ä»¶è·¯å¾„å¹¶åŠ è½½
env_path = Path(__file__).parent / '.env'
load_dotenv(dotenv_path=env_path)

# å®šä¹‰ lifespan ä¸Šä¸‹æ–‡ç®¡ç†å™¨
@asynccontextmanager
async def lifespan(app: FastAPI):
    # åº”ç”¨å¯åŠ¨æ—¶
    logger.info("ğŸš€ åº”ç”¨å¯åŠ¨ä¸­...")
    await connect_to_mongo()
    yield
    # åº”ç”¨å…³é—­æ—¶
    logger.info("ğŸ›‘ åº”ç”¨å…³é—­ä¸­...")
    await close_mongo_connection()

# åˆ›å»º FastAPI åº”ç”¨å®ä¾‹ï¼Œä½¿ç”¨ lifespan
app = FastAPI(
    title="å°çº¢ä¹¦çˆ¬è™« API",
    version="1.0.0",
    description="ä¸€ä¸ªç”¨äºåˆ†æå°çº¢ä¹¦ç”¨æˆ·å’Œç¬”è®°çš„API",
    lifespan=lifespan
)

# è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°
def safe_int(value, default=0):
    """
    å®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæ•´æ•°ï¼Œæ”¯æŒä¸­æ–‡æ•°å­—æ ¼å¼ï¼ˆå¦‚ "2.7ä¸‡"ï¼‰
    :param value: è¦è½¬æ¢çš„å€¼ï¼ˆå¯èƒ½æ˜¯ int, str, None ç­‰ï¼‰
    :param default: è½¬æ¢å¤±è´¥æ—¶çš„é»˜è®¤å€¼
    :return: æ•´æ•°å€¼
    """
    if value is None:
        return default
    
    try:
        # å¦‚æœå·²ç»æ˜¯æ•´æ•°ï¼Œç›´æ¥è¿”å›
        if isinstance(value, int):
            return value
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå¤„ç†ä¸­æ–‡æ•°å­—æ ¼å¼
        if isinstance(value, str):
            value = value.strip()
            
            # å¤„ç†ç©ºå­—ç¬¦ä¸²
            if not value:
                return default
            
            # å¤„ç†åŒ…å«"ä¸‡"çš„æƒ…å†µï¼ˆå¦‚ "2.7ä¸‡" = 27000ï¼‰
            if 'ä¸‡' in value:
                num_str = value.replace('ä¸‡', '').strip()
                try:
                    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å†ä¹˜ä»¥10000
                    return int(float(num_str) * 10000)
                except ValueError:
                    return default
            
            # å¤„ç†åŒ…å«"åƒ"çš„æƒ…å†µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'åƒ' in value:
                num_str = value.replace('åƒ', '').strip()
                try:
                    return int(float(num_str) * 1000)
                except ValueError:
                    return default
            
            # æ™®é€šæ•°å­—å­—ç¬¦ä¸²
            return int(float(value))
        
        # å…¶ä»–ç±»å‹å°è¯•ç›´æ¥è½¬æ¢
        return int(value)
        
    except (ValueError, TypeError):
        return default

#å®šä¹‰
class Data_Spider():
    def __init__(self):
        self.xhs_apis = XHS_Apis()

    def fetch_user_detailed_info(self, user_id: str, user_name: str, cookies_str: str, proxies=None):
        """
        è·å–ç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç²‰ä¸æ•°
        é€šè¿‡æœç´¢ç”¨æˆ·åæ¥è·å–åŒ…å«ç²‰ä¸æ•°çš„æ•°æ®
        """
        try:
            # ä½¿ç”¨ search_user API æœç´¢ç”¨æˆ·å
            success, msg, search_result = self.xhs_apis.search_user(user_name, cookies_str, page=1, proxies=proxies)
            
            if not success or not search_result.get('data', {}).get('users'):
                logger.warning(f"æœç´¢ç”¨æˆ· {user_name} å¤±è´¥: {msg}")
                return None, False, msg
            
            # åœ¨æœç´¢ç»“æœä¸­æ‰¾åˆ°åŒ¹é…çš„ç”¨æˆ·
            users = search_result['data']['users']
            target_user = None
            
            for user in users:
                if user.get('id') == user_id or user.get('name') == user_name:
                    target_user = user
                    break
            
            if not target_user and len(users) > 0:
                # å¦‚æœæ²¡æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç»“æœï¼ˆå¯èƒ½æ˜¯æœ€ç›¸å…³çš„ï¼‰
                target_user = users[0]
                logger.warning(f"æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„ç”¨æˆ·ï¼Œä½¿ç”¨æœç´¢ç»“æœç¬¬ä¸€ä¸ª: {target_user.get('name')}")
            
            if not target_user:
                return None, False, "æœªæ‰¾åˆ°ç”¨æˆ·ä¿¡æ¯"
            
            # æå–æœ‰ç”¨çš„å­—æ®µ
            user_detail = {
                "user_id": target_user.get('id'),
                "user_name": target_user.get('name'),
                "red_id": target_user.get('red_id'),  # å°çº¢ä¹¦å·
                "fans": target_user.get('fans', '0'),  # ç²‰ä¸æ•°ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¦‚ "2.7ä¸‡"ï¼‰
                "note_count": target_user.get('note_count', 0),  # ç¬”è®°æ•°é‡
                "is_verified": target_user.get('red_official_verified', False),  # æ˜¯å¦è®¤è¯
                "avatar": target_user.get('image', ''),  # å¤´åƒ
            }
            
            return user_detail, True, 'æˆåŠŸè·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯'
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ· {user_name} è¯¦ç»†ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None, False, f"æœªçŸ¥é”™è¯¯: {e}"

    def fetch_user_all_notes(self, user_url: str, cookies_str: str, proxies=None):
        """
        ä¸€ä¸ªåªè´Ÿè´£æŠ“å–æ•°æ®å¹¶è¿”å›ï¼Œä¸è¿›è¡Œä»»ä½•æ–‡ä»¶ä¿å­˜çš„æ–¹æ³•ã€‚
        """
        try:
            success, msg, all_note_info = self.xhs_apis.get_user_all_notes(user_url, cookies_str, proxies)
            if not success:
                logger.warning(f"APIè°ƒç”¨è·å–ç”¨æˆ·ç¬”è®°å¤±è´¥: {msg}")
                return [], False, msg

            note_list = []
            logger.info(f'ç”¨æˆ· {user_url} ä½œå“æ•°é‡: {len(all_note_info)}')
            for simple_note_info in all_note_info:
                note_url = f"https://www.xiaohongshu.com/explore/{simple_note_info['note_id']}?xsec_token={simple_note_info['xsec_token']}"
                note_list.append(note_url)

            return note_list, True, f'æˆåŠŸè·å– {len(note_list)} æ¡ç¬”è®°é“¾æ¥'

        except Exception as e:
            logger.error(f"çˆ¬å–ç”¨æˆ· {user_url} æ‰€æœ‰ç¬”è®°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return [], False, f"æœªçŸ¥é”™è¯¯: {e}"
   
    def fetch_note_details(self, note_url: str, cookies_str: str, proxies=None):
        """
        æ ¹æ®ç¬”è®°URLè·å–å•ç¯‡ç¬”è®°çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶æ ¼å¼åŒ–ä¸º content_snapshot ç»“æ„
        """
        try:
            # 1. è·å–ç¬”è®°çš„åŸºç¡€ä¿¡æ¯
            success, msg, note_info_response = self.xhs_apis.get_note_info(note_url, cookies_str, proxies)
            if not success:
                logger.warning(f"è·å–ç¬”è®° {note_url} è¯¦æƒ…å¤±è´¥: {msg}")
                return None, False, msg

            # å®‰å…¨æ£€æŸ¥æ•°æ®ç»“æ„
            if not note_info_response.get('data', {}).get('items'):
                logger.warning(f"ç¬”è®° {note_url} æ•°æ®æ ¼å¼å¼‚å¸¸")
                return None, False, "ç¬”è®°æ•°æ®æ ¼å¼å¼‚å¸¸"

            note_data = note_info_response['data']['items'][0]
            note_card = note_data.get('note_card', {})
            
            # æå–åŸºç¡€å­—æ®µ
            note_id = note_data.get("id")
            if not note_id:
                logger.warning(f"ç¬”è®° {note_url} ç¼ºå°‘note_id")
                return None, False, "ç¬”è®°IDè·å–å¤±è´¥"

            # æå–ç”¨æˆ·ä¿¡æ¯
            user = note_card.get('user', {})
            user_id = user.get('user_id', user.get('id', ''))
            user_nickname = user.get('nickname', '')
            
            # æå–ç¬”è®°ç±»å‹
            note_type = note_card.get('type', 'normal')  # 'normal' æˆ– 'video'
            content_type = 'video' if note_type == 'video' else 'note'
            
            # æå–æ ‡é¢˜å’Œæè¿°
            title = note_card.get('title', '')
            description = note_card.get('desc', '')
            
            # æå–å‘å¸ƒæ—¶é—´
            published_time = note_card.get('time', 0)
            if published_time:
                published_time = datetime.datetime.fromtimestamp(published_time / 1000, tz=datetime.timezone.utc)
            
            # æå–äº’åŠ¨æ•°æ®ï¼ˆä½¿ç”¨ safe_int è½¬æ¢ï¼‰
            interact_info = note_card.get('interact_info', {})
            liked_count = safe_int(interact_info.get('liked_count', 0))
            collected_count = safe_int(interact_info.get('collected_count', 0))
            comment_count = safe_int(interact_info.get('comment_count', 0))
            share_count = safe_int(interact_info.get('share_count', 0))
            
            # æå–æ ‡ç­¾
            tag_list = note_card.get('tag_list', [])
            tags = [tag.get('name', '') for tag in tag_list if tag.get('name')]
            
            # 2. è·å–ç¬”è®°çš„è¯„è®ºä¿¡æ¯
            comments_success, comments_msg, comments_data = self.xhs_apis.get_note_all_comment(note_url, cookies_str, proxies)
            if not comments_success:
                logger.warning(f"è·å–ç¬”è®° {note_url} è¯„è®ºå¤±è´¥: {comments_msg}")
                comments_list = []
            else:
                comments_list = comments_data if isinstance(comments_data, list) else []

            # 3. æ ¼å¼åŒ–è¯„è®ºæ•°æ®ä¸º content_snapshot ç»“æ„
            formatted_comments = []
            for comment in comments_list:
                formatted_comment = {
                    "commenter_id": comment.get('user_info', {}).get('user_id', ''),
                    "commenter_name": comment.get('user_info', {}).get('nickname', ''),
                    "comment_content": comment.get('content', ''),
                    "published_time": comment.get('create_time', 0),
                    "likes_on_comment": safe_int(comment.get('like_count', 0))
                }
                formatted_comments.append(formatted_comment)

            # 4. æ„å»ºæœ€ç»ˆçš„ content_snapshot ç»“æ„
            content_snapshot = {
                "channel_id": user_id,  # ç”¨æˆ·IDä½œä¸ºchannel_id
                "content_id": note_id,
                "content_type": content_type,
                "content_title": title,
                "likes": liked_count,
                "shares": share_count,
                "views": 0,  # å°çº¢ä¹¦APIä¸æä¾›æµè§ˆé‡
                "published_time": published_time,
                "collected_number": collected_count,
                "comments": formatted_comments,
                "description": description,
                "tags": tags,
                "note_url": note_url,
                "last_updated": datetime.datetime.now(datetime.timezone.utc),
                # ä¿ç•™åŸå§‹æ•°æ®ä»¥å¤‡æŸ¥
                "_raw_data": {
                    "note_card": note_card,
                    "comment_count": comment_count
                }
            }
            
            logger.info(f"æˆåŠŸè§£æç¬”è®° {note_id}: likes={liked_count}, collects={collected_count}, comments={len(formatted_comments)}")
            
            return content_snapshot, True, 'æˆåŠŸè·å–ç¬”è®°è¯¦æƒ…'

        except Exception as e:
            logger.error(f"å¤„ç†ç¬”è®° {note_url} è¯¦æƒ…æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None, False, f"å¤„ç†æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"

data_spider = Data_Spider() 

# --- 3. å®šä¹‰ API çš„è¾“å…¥/è¾“å‡ºæ¨¡å‹ (Pydantic) ---
class UserNotesRequest(BaseModel):
    user_url: str = Field(..., description="å°çº¢ä¹¦ç”¨æˆ·çš„ä¸ªäººä¸»é¡µURL")
    cookies: str = Field(..., description="ç”¨äºè®¤è¯çš„Cookieå­—ç¬¦ä¸²")
    proxies: Optional[Dict[str, str]] = None

class StandardResponse(BaseModel):
    success: bool
    message: str
    data: Optional[List] = None
# --- 4. åˆ›å»º API ç«¯ç‚¹ (Endpoint) ---
@app.get('/user/notes', response_model=StandardResponse, summary="è·å–ç”¨æˆ·ç¬”è®°")
async def get_user_notes_api(user_url: str):
    """
    ä¼ å…¥ç”¨æˆ·URLï¼Œè¿”å›ç”¨æˆ·ç¬”è®°é“¾æ¥åˆ—è¡¨åŠç”¨æˆ·è¯¦ç»†ä¿¡æ¯
    """
    import urllib.parse
    
    cookies = os.getenv("COOKIES")
    
    # 1. ä»URLæå–user_id
    urlParse = urllib.parse.urlparse(user_url)
    user_id = urlParse.path.split("/")[-1]
    
    # 2. è·å–ç”¨æˆ·åŸºç¡€ä¿¡æ¯
    success_info, msg_info, user_info_response = data_spider.xhs_apis.get_user_info(user_id, cookies)
    user_name = "æœªçŸ¥ç”¨æˆ·"
    if success_info and user_info_response.get('data', {}).get('basic_info'):
        basic_info = user_info_response['data']['basic_info']
        user_name = basic_info.get('nickname', 'æœªçŸ¥ç”¨æˆ·')
    
    # 3. è·å–ç”¨æˆ·ç¬”è®°åˆ—è¡¨
    user_notes, success, msg = data_spider.fetch_user_all_notes(user_url, cookies)

    if not success:
        raise HTTPException(status_code=400, detail=f"çˆ¬å–å¤±è´¥: {msg}")
    
    # 4. è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç²‰ä¸æ•°ï¼‰
    user_detail, success_detail, msg_detail = data_spider.fetch_user_detailed_info(user_id, user_name, cookies)
    
    # 5. è®¡ç®—æ€»ç‚¹èµæ•°ï¼ˆä»ç¬”è®°åˆ—è¡¨ä¸­ç´¯åŠ ï¼‰
    kvs = urlParse.query.split('&')
    kvDist = {kv.split('=')[0]: kv.split('=')[1] for kv in kvs}
    xsec_token = kvDist.get('xsec_token', '')
    xsec_source = kvDist.get('xsec_source', 'pc_search')
    
    total_likes = 0
    total_collects = 0
    total_comments = 0
    
    # è·å–ç¬”è®°è¯¦ç»†ä¿¡æ¯ä»¥ç´¯åŠ äº’åŠ¨æ•°æ®
    success_notes, msg_notes, res_json = data_spider.xhs_apis.get_user_note_info(
        user_id, '', cookies, xsec_token, xsec_source
    )
    
    if success_notes and res_json and res_json.get('data', {}).get('notes'):
        notes = res_json['data']['notes']
        for note in notes:
            interact_info = note.get('interact_info', {})
            # ä½¿ç”¨è¾…åŠ©å‡½æ•°å®‰å…¨åœ°è½¬æ¢å¹¶ç´¯åŠ 
            total_likes += safe_int(interact_info.get('liked_count'))
            total_collects += safe_int(interact_info.get('collected_count'))
            total_comments += safe_int(interact_info.get('comment_count'))
    
    db = get_database()
    
    # å‡†å¤‡å®Œæ•´çš„ç”¨æˆ·ä¿¡æ¯æ–‡æ¡£
    if db is not None and user_detail:
        try:
            # è·å–åä¸º "users" çš„é›†åˆ (collection)
            user_collection = db.get_collection("users")
            
            # å‡†å¤‡è¦å­˜å…¥/æ›´æ–°çš„å®Œæ•´æ•°æ®æ–‡æ¡£
            user_document = {
                "user_id": user_id,
                "user_url": user_url,
                "user_name": user_detail.get('user_name', user_name),
                "red_id": user_detail.get('red_id', ''),
                "fans": user_detail.get('fans', '0'), 
                "fans_count": safe_int(user_detail.get('fans', '0')),  # è½¬æ¢ä¸ºæ•°å­—ä¾¿äºæŸ¥è¯¢
                "avatar": user_detail.get('avatar', ''),
                "is_verified": user_detail.get('is_verified', False),
                "note_urls": user_notes,
                "note_count": len(user_notes),
                "total_likes": total_likes,
                "last_updated": datetime.datetime.now(datetime.timezone.utc)
            }

            # ä½¿ç”¨ update_one + upsert=True æ¥æ’å…¥æˆ–æ›´æ–°ç”¨æˆ·ä¿¡æ¯
            await user_collection.update_one(
                {'user_id': user_id},
                {'$set': user_document},
                upsert=True
            )
            logger.info(f"ç”¨æˆ· {user_id} ({user_document['user_name']}) çš„å®Œæ•´ä¿¡æ¯å·²æˆåŠŸå­˜å‚¨åˆ° MongoDBã€‚")
            logger.info(f"ç”¨æˆ·æ•°æ®: ç²‰ä¸={user_document['fans']}, ç¬”è®°æ•°={user_document['note_count']}, æ€»ç‚¹èµ={total_likes}")
        except Exception as e:
            # å³ä½¿å­˜å‚¨å¤±è´¥ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥è¿”å›æ•°æ®ç»™ç”¨æˆ·ï¼Œåªè®°å½•é”™è¯¯
            logger.error(f"å­˜å‚¨ç”¨æˆ· {user_url} åˆ° MongoDB æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    return {
        "success": success,
        "message": msg,
        "data": user_notes
    }

@app.get('/note/info', summary="è·å–ç¬”è®°è¯¦æƒ…")
async def get_note_details_api(note_url: str):
    """
    ä¼ å…¥ç¬”è®°urlï¼Œè¿”å›ç¬”è®°çš„è¯¦ç»†ä¿¡æ¯ï¼ˆcontent_snapshot æ ¼å¼ï¼‰
    """
    cookies = os.getenv("COOKIES")
    content_snapshot, success, msg = data_spider.fetch_note_details(note_url, cookies)
    if not success:
        raise HTTPException(status_code=400, detail=f"çˆ¬å–å¤±è´¥: {msg}")
    
    db = get_database()
    
    # ä¿®å¤ï¼šä½¿ç”¨ is not None æ£€æŸ¥
    if db is not None and content_snapshot:
        try:
            # ç²å–åç‚º "notes" çš„é›†åˆ (collection)
            note_collection = db.get_collection("notes")
            
            # ä½¿ç”¨ content_id ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œupdate_one + upsert=True ä¾†é¿å…æ’å…¥é‡è¤‡çš„ç­†è¨˜
            await note_collection.update_one(
                {'content_id': content_snapshot['content_id']},
                {'$set': content_snapshot},
                upsert=True
            )
            logger.info(f"ç¬”è®° {content_snapshot['content_id']} å·²æˆåŠŸå­˜å‚¨åˆ° MongoDB (content_snapshot æ ¼å¼)")
            logger.info(f"äº’åŠ¨æ•°æ®: likes={content_snapshot['likes']}, collects={content_snapshot['collected_number']}, comments={len(content_snapshot['comments'])}")
        except Exception as e:
            # å³ä½¿å„²å­˜å¤±æ•—ï¼Œæˆ‘å€‘ä»ç„¶å¯ä»¥å›å‚³è³‡æ–™çµ¦ä½¿ç”¨è€…ï¼Œåªè¨˜éŒ„éŒ¯èª¤
            logger.error(f"å„²å­˜ç­†è¨˜ {content_snapshot.get('content_id')} åˆ° MongoDB æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            logger.error(traceback.format_exc())

    return {
        "success": success,
        "message": msg,
        "data": content_snapshot
    }
