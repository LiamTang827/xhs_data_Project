"""
GitHub Actions å®šæ—¶çˆ¬è™«æ‰§è¡Œè„šæœ¬

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
2. çˆ¬å–æŒ‡å®šç”¨æˆ·çš„æ•°æ®
3. çˆ¬å–ç¬”è®°è¯¦æƒ…
4. å­˜å‚¨åˆ° MongoDB
5. ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š
"""

import os
import sys
import asyncio
import datetime
from loguru import logger
from typing import List, Dict
import json

# é…ç½®æ—¥å¿—
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)

# é…ç½® loguru è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)
logger.add(
    f"{log_dir}/spider_{{time:YYYY-MM-DD}}.log",
    rotation="500 MB",
    retention="30 days",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="DEBUG"
)

from apis.xhs_pc_apis import XHS_Apis
from main import Data_Spider, safe_int, get_database
from dotenv import load_dotenv
import motor.motor_asyncio

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é»˜è®¤è¦çˆ¬å–çš„ç”¨æˆ·URLåˆ—è¡¨ï¼ˆå¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼‰
DEFAULT_USER_URLS = [
    # åœ¨è¿™é‡Œæ·»åŠ ä½ æƒ³è¦å®šæ—¶çˆ¬å–çš„ç”¨æˆ·URL
    # "https://www.xiaohongshu.com/user/profile/xxx?xsec_token=xxx",
]

class SpiderRunner:
    """çˆ¬è™«æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.data_spider = Data_Spider()
        self.cookies = os.getenv("COOKIES")
        self.mongo_uri = os.getenv("MONGO_URI")
        
        if not self.cookies:
            raise ValueError("âŒ COOKIES ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")
        if not self.mongo_uri:
            raise ValueError("âŒ MONGO_URI ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")
        
        # åˆå§‹åŒ– MongoDB è¿æ¥
        self.db_client = motor.motor_asyncio.AsyncIOMotorClient(self.mongo_uri)
        self.database = self.db_client["xhs_data"]
        self.user_collection = self.database.get_collection("users")
        self.note_collection = self.database.get_collection("notes")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "start_time": datetime.datetime.now(datetime.timezone.utc),
            "users_processed": 0,
            "users_failed": 0,
            "notes_processed": 0,
            "notes_failed": 0,
            "errors": []
        }
    
    def get_user_urls(self) -> List[str]:
        """è·å–è¦çˆ¬å–çš„ç”¨æˆ·URLåˆ—è¡¨"""
        # 1. ä»ç¯å¢ƒå˜é‡è·å–ï¼ˆç”¨äºæ‰‹åŠ¨è§¦å‘ï¼‰
        env_urls = os.getenv("USER_URLS", "").strip()
        if env_urls:
            urls = [url.strip() for url in env_urls.split(",") if url.strip()]
            logger.info(f"ä»ç¯å¢ƒå˜é‡è·å–åˆ° {len(urls)} ä¸ªç”¨æˆ·URL")
            return urls
        
        # 2. ä»é…ç½®æ–‡ä»¶è·å–
        config_file = "spider_config.json"
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    urls = config.get("user_urls", [])
                    logger.info(f"ä»é…ç½®æ–‡ä»¶è·å–åˆ° {len(urls)} ä¸ªç”¨æˆ·URL")
                    return urls
            except Exception as e:
                logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        
        # 3. ä½¿ç”¨é»˜è®¤åˆ—è¡¨
        if DEFAULT_USER_URLS:
            logger.info(f"ä½¿ç”¨é»˜è®¤ç”¨æˆ·URLåˆ—è¡¨ ({len(DEFAULT_USER_URLS)} ä¸ª)")
            return DEFAULT_USER_URLS
        
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç”¨æˆ·URLï¼Œå°†ä¸æ‰§è¡Œçˆ¬å–")
        return []
    
    async def crawl_user(self, user_url: str) -> Dict:
        """çˆ¬å–å•ä¸ªç”¨æˆ·çš„æ•°æ®"""
        try:
            logger.info(f"=" * 60)
            logger.info(f"å¼€å§‹çˆ¬å–ç”¨æˆ·: {user_url}")
            
            import urllib.parse
            
            # 1. ä»URLæå–user_id
            urlParse = urllib.parse.urlparse(user_url)
            user_id = urlParse.path.split("/")[-1]
            
            # 2. è·å–ç”¨æˆ·åŸºç¡€ä¿¡æ¯
            success_info, msg_info, user_info_response = self.data_spider.xhs_apis.get_user_info(user_id, self.cookies)
            user_name = "æœªçŸ¥ç”¨æˆ·"
            if success_info and user_info_response.get('data', {}).get('basic_info'):
                basic_info = user_info_response['data']['basic_info']
                user_name = basic_info.get('nickname', 'æœªçŸ¥ç”¨æˆ·')
            
            logger.info(f"ç”¨æˆ·å: {user_name}")
            
            # 3. è·å–ç”¨æˆ·ç¬”è®°åˆ—è¡¨
            user_notes, success, msg = self.data_spider.fetch_user_all_notes(user_url, self.cookies)
            
            if not success:
                logger.error(f"è·å–ç”¨æˆ·ç¬”è®°å¤±è´¥: {msg}")
                self.stats["users_failed"] += 1
                self.stats["errors"].append(f"ç”¨æˆ· {user_name}: {msg}")
                return {"success": False, "user_name": user_name, "error": msg}
            
            logger.info(f"è·å–åˆ° {len(user_notes)} æ¡ç¬”è®°")
            
            # 4. è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ç²‰ä¸æ•°ï¼‰
            user_detail, success_detail, msg_detail = self.data_spider.fetch_user_detailed_info(
                user_id, user_name, self.cookies
            )
            
            # 5. è®¡ç®—æ€»äº’åŠ¨æ•°
            kvs = urlParse.query.split('&') if urlParse.query else []
            kvDist = {kv.split('=')[0]: kv.split('=')[1] for kv in kvs if '=' in kv}
            xsec_token = kvDist.get('xsec_token', '')
            xsec_source = kvDist.get('xsec_source', 'pc_search')
            
            total_likes = 0
            total_collects = 0
            total_comments = 0
            
            success_notes, msg_notes, res_json = self.data_spider.xhs_apis.get_user_note_info(
                user_id, '', self.cookies, xsec_token, xsec_source
            )
            
            if success_notes and res_json and res_json.get('data', {}).get('notes'):
                notes = res_json['data']['notes']
                for note in notes:
                    interact_info = note.get('interact_info', {})
                    total_likes += safe_int(interact_info.get('liked_count'))
                    total_collects += safe_int(interact_info.get('collected_count'))
                    total_comments += safe_int(interact_info.get('comment_count'))
            
            fan_count = safe_int(user_detail.get('fans', '0')) if user_detail else 0
            
            # 6. å­˜å‚¨åˆ° MongoDB
            user_snapshot = {
                "last_updated": datetime.datetime.now(datetime.timezone.utc),
                "user_url": user_url,
                "red_id": user_detail.get('red_id', '') if user_detail else '',
                "user_name": user_name,
                "note_count": len(user_notes),
                "note_urls": user_notes,
                "fan_count": fan_count,
                "total_likes": total_likes,
                "total_collects": total_collects,
                "total_comments": total_comments,
            }
            
            await self.user_collection.insert_one(user_snapshot)
            logger.success(f"âœ… ç”¨æˆ·æ•°æ®å·²å­˜å‚¨: {user_name} | ç²‰ä¸: {fan_count} | ç¬”è®°: {len(user_notes)}")
            
            self.stats["users_processed"] += 1
            
            return {
                "success": True,
                "user_name": user_name,
                "note_count": len(user_notes),
                "note_urls": user_notes,
                "fan_count": fan_count
            }
            
        except Exception as e:
            logger.error(f"çˆ¬å–ç”¨æˆ· {user_url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self.stats["users_failed"] += 1
            self.stats["errors"].append(f"ç”¨æˆ· {user_url}: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def crawl_note(self, note_url: str) -> Dict:
        """çˆ¬å–å•ç¯‡ç¬”è®°çš„è¯¦æƒ…"""
        try:
            content_snapshot, success, msg = self.data_spider.fetch_note_details(note_url, self.cookies)
            
            if not success:
                logger.warning(f"è·å–ç¬”è®°å¤±è´¥: {note_url} - {msg}")
                self.stats["notes_failed"] += 1
                return {"success": False, "note_url": note_url, "error": msg}
            
            # å­˜å‚¨åˆ° MongoDB
            await self.note_collection.update_one(
                {'content_id': content_snapshot['content_id']},
                {'$set': content_snapshot},
                upsert=True
            )
            
            logger.info(f"âœ… ç¬”è®°å·²å­˜å‚¨: {content_snapshot['content_id']} | ç‚¹èµ: {content_snapshot['likes']}")
            self.stats["notes_processed"] += 1
            
            return {"success": True, "content_id": content_snapshot['content_id']}
            
        except Exception as e:
            logger.error(f"çˆ¬å–ç¬”è®° {note_url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            self.stats["notes_failed"] += 1
            return {"success": False, "note_url": note_url, "error": str(e)}
    
    async def run(self):
        """æ‰§è¡Œçˆ¬è™«ä»»åŠ¡"""
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®šæ—¶çˆ¬è™«ä»»åŠ¡")
        logger.info(f"æ‰§è¡Œæ—¶é—´: {self.stats['start_time']}")
        logger.info("=" * 60)
        
        # 1. è·å–è¦çˆ¬å–çš„ç”¨æˆ·åˆ—è¡¨
        user_urls = self.get_user_urls()
        
        if not user_urls:
            logger.warning("âš ï¸ æ²¡æœ‰è¦çˆ¬å–çš„ç”¨æˆ·ï¼Œä»»åŠ¡ç»“æŸ")
            return
        
        logger.info(f"å…±éœ€çˆ¬å– {len(user_urls)} ä¸ªç”¨æˆ·")
        
        # 2. çˆ¬å–ç”¨æˆ·æ•°æ®
        user_results = []
        for i, user_url in enumerate(user_urls, 1):
            logger.info(f"\nè¿›åº¦: {i}/{len(user_urls)}")
            result = await self.crawl_user(user_url)
            user_results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬
            if i < len(user_urls):
                await asyncio.sleep(3)
        
        # 3. æ˜¯å¦çˆ¬å–ç¬”è®°è¯¦æƒ…
        crawl_notes = os.getenv("CRAWL_NOTES", "true").lower() == "true"
        
        if crawl_notes:
            logger.info("\n" + "=" * 60)
            logger.info("å¼€å§‹çˆ¬å–ç¬”è®°è¯¦æƒ…")
            logger.info("=" * 60)
            
            # æ”¶é›†æ‰€æœ‰ç¬”è®°URL
            all_note_urls = []
            for result in user_results:
                if result.get("success") and result.get("note_urls"):
                    all_note_urls.extend(result["note_urls"])
            
            logger.info(f"å…±éœ€çˆ¬å– {len(all_note_urls)} ç¯‡ç¬”è®°")
            
            # çˆ¬å–ç¬”è®°ï¼ˆé™åˆ¶æ•°é‡ï¼Œé¿å…è¶…æ—¶ï¼‰
            max_notes = int(os.getenv("MAX_NOTES_PER_RUN", "50"))
            notes_to_crawl = all_note_urls[:max_notes]
            
            if len(all_note_urls) > max_notes:
                logger.warning(f"âš ï¸ ç¬”è®°æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œæœ¬æ¬¡ä»…çˆ¬å–å‰ {max_notes} ç¯‡")
            
            for i, note_url in enumerate(notes_to_crawl, 1):
                logger.info(f"\nç¬”è®°è¿›åº¦: {i}/{len(notes_to_crawl)}")
                await self.crawl_note(note_url)
                
                # æ·»åŠ å»¶è¿Ÿ
                if i < len(notes_to_crawl):
                    await asyncio.sleep(2)
        
        # 4. ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š"""
        end_time = datetime.datetime.now(datetime.timezone.utc)
        duration = end_time - self.stats["start_time"]
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æ‰§è¡ŒæŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"å¼€å§‹æ—¶é—´: {self.stats['start_time']}")
        logger.info(f"ç»“æŸæ—¶é—´: {end_time}")
        logger.info(f"æ‰§è¡Œæ—¶é•¿: {duration}")
        logger.info(f"")
        logger.info(f"ç”¨æˆ·ç»Ÿè®¡:")
        logger.info(f"  - æˆåŠŸ: {self.stats['users_processed']}")
        logger.info(f"  - å¤±è´¥: {self.stats['users_failed']}")
        logger.info(f"")
        logger.info(f"ç¬”è®°ç»Ÿè®¡:")
        logger.info(f"  - æˆåŠŸ: {self.stats['notes_processed']}")
        logger.info(f"  - å¤±è´¥: {self.stats['notes_failed']}")
        
        if self.stats["errors"]:
            logger.info(f"")
            logger.warning(f"é”™è¯¯åˆ—è¡¨ ({len(self.stats['errors'])} ä¸ª):")
            for error in self.stats["errors"]:
                logger.warning(f"  - {error}")
        
        logger.info("=" * 60)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report = {
            "start_time": self.stats["start_time"].isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration.total_seconds(),
            "users_processed": self.stats["users_processed"],
            "users_failed": self.stats["users_failed"],
            "notes_processed": self.stats["notes_processed"],
            "notes_failed": self.stats["notes_failed"],
            "errors": self.stats["errors"]
        }
        
        report_file = f"{log_dir}/report_{self.stats['start_time'].strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        runner = SpiderRunner()
        await runner.run()
        
        # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
        if runner.stats["users_failed"] > 0 or runner.stats["notes_failed"] > 0:
            logger.warning("âš ï¸ éƒ¨åˆ†ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            sys.exit(1)
        else:
            logger.success("âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
            sys.exit(0)
            
    except Exception as e:
        logger.error(f"âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
