name: å®šæ—¶çˆ¬å–å°çº¢ä¹¦æ•°æ®

# è§¦å‘æ¡ä»¶
on:
  # å®šæ—¶è§¦å‘ï¼ˆä½¿ç”¨ cron è¡¨è¾¾å¼ï¼‰
  schedule:
    # æ¯å¤© UTC æ—¶é—´ 00:00 æ‰§è¡Œï¼ˆåŒ—äº¬æ—¶é—´ 08:00ï¼‰
    - cron: '0 0 * * *'
    # æ¯12å°æ—¶æ‰§è¡Œä¸€æ¬¡
    # - cron: '0 */12 * * *'
    # æ¯å‘¨ä¸€æ—©ä¸Š8ç‚¹ï¼ˆUTC 00:00ï¼‰
    # - cron: '0 0 * * 1'
  
  # æ‰‹åŠ¨è§¦å‘ï¼ˆå¯ä»¥åœ¨ GitHub Actions é¡µé¢æ‰‹åŠ¨è¿è¡Œï¼‰
  workflow_dispatch:
    inputs:
      user_urls:
        description: 'è¦çˆ¬å–çš„ç”¨æˆ·URLåˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰'
        required: false
        default: ''
      note_urls:
        description: 'è¦çˆ¬å–çš„ç¬”è®°URLåˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰'
        required: false
        default: ''

# ç¯å¢ƒå˜é‡ï¼ˆä» GitHub Secrets ä¸­è¯»å–ï¼‰
env:
  COOKIES: ${{ secrets.COOKIES }}
  MONGO_URI: ${{ secrets.MONGO_URI }}

jobs:
  spider-job:
    name: æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
    runs-on: ubuntu-latest
    
    # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆ30åˆ†é’Ÿï¼‰
    timeout-minutes: 30
    
    steps:
      # 1. æ£€å‡ºä»£ç 
      - name: ğŸ“¥ æ£€å‡ºä»£ç 
        uses: actions/checkout@v4
      
      # 2. è®¾ç½® Python ç¯å¢ƒ
      - name: ğŸ è®¾ç½® Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: spider-api/requirements.txt
      
      # 3. å®‰è£… Chrome æµè§ˆå™¨ï¼ˆSelenium éœ€è¦ï¼‰
      - name: ğŸŒ å®‰è£… Chrome æµè§ˆå™¨
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
      
      # 4. å®‰è£… ChromeDriver
      - name: ğŸš— å®‰è£… ChromeDriver
        uses: nanasess/setup-chromedriver@v2
      
      # 5. éªŒè¯ Chrome å’Œ ChromeDriver
      - name: âœ… éªŒè¯æµè§ˆå™¨ç¯å¢ƒ
        run: |
          google-chrome --version
          chromedriver --version
      
      # 6. å®‰è£… Python ä¾èµ–
      - name: ğŸ“¦ å®‰è£…ä¾èµ–
        working-directory: ./spider-api
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      # 7. åˆ›å»º .env æ–‡ä»¶
      - name: ğŸ“ é…ç½®ç¯å¢ƒå˜é‡
        working-directory: ./spider-api
        run: |
          echo "COOKIES='${{ secrets.COOKIES }}'" > .env
          echo "MONGO_URI='${{ secrets.MONGO_URI }}'" >> .env
          echo "ç¯å¢ƒå˜é‡å·²é…ç½®"
      
      # 8. åˆ›å»ºçˆ¬è™«æ‰§è¡Œè„šæœ¬
      - name: ğŸ“œ åˆ›å»ºæ‰§è¡Œè„šæœ¬
        working-directory: ./spider-api
        run: |
          cat > run_spider.py << 'EOF'
          import asyncio
          import os
          import sys
          from loguru import logger
          from app.service import SpiderService
          from app.xhs_utils.database import connect_to_mongo, close_mongo_connection
          
          # é…ç½®æ—¥å¿—
          logger.add("spider_cron.log", rotation="10 MB", level="INFO")
          
          async def main():
              """ä¸»æ‰§è¡Œå‡½æ•°"""
              try:
                  # 1. è¿æ¥æ•°æ®åº“
                  logger.info("ğŸ”Œ æ­£åœ¨è¿æ¥æ•°æ®åº“...")
                  await connect_to_mongo()
                  
                  # 2. åˆ›å»º Service å®ä¾‹
                  service = SpiderService()
                  cookies = os.getenv("COOKIES")
                  
                  if not cookies:
                      logger.error("âŒ COOKIES ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼")
                      sys.exit(1)
                  
                  # 3. è¯»å–è¦çˆ¬å–çš„ URLï¼ˆä»ç¯å¢ƒå˜é‡æˆ– GitHub Workflow è¾“å…¥ï¼‰
                  user_urls_str = os.getenv("USER_URLS", "")
                  note_urls_str = os.getenv("NOTE_URLS", "")
                  
                  # å¦‚æœæ²¡æœ‰æŒ‡å®š URLï¼Œè¿™é‡Œå¯ä»¥ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“è¯»å–
                  # æˆ–è€…ä½¿ç”¨é»˜è®¤çš„æµ‹è¯• URL
                  if not user_urls_str and not note_urls_str:
                      logger.warning("âš ï¸ æœªæŒ‡å®šè¦çˆ¬å–çš„ URLï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶...")
                      # TODO: ä»é…ç½®æ–‡ä»¶æˆ–æ•°æ®åº“è¯»å–è¦çˆ¬å–çš„ URL åˆ—è¡¨
                      user_urls_str = ""  # è¿™é‡Œå¯ä»¥è®¾ç½®é»˜è®¤ URL
                  
                  # 4. å¤„ç†ç”¨æˆ· URL
                  if user_urls_str:
                      user_urls = [url.strip() for url in user_urls_str.split(",") if url.strip()]
                      logger.info(f"ğŸ“‹ å‡†å¤‡çˆ¬å– {len(user_urls)} ä¸ªç”¨æˆ·")
                      
                      for idx, user_url in enumerate(user_urls, 1):
                          try:
                              logger.info(f"[{idx}/{len(user_urls)}] æ­£åœ¨å¤„ç†ç”¨æˆ·: {user_url}")
                              user_data = await service.process_user_data(user_url, cookies)
                              logger.success(f"âœ… ç”¨æˆ·æ•°æ®å·²ä¿å­˜: {user_data.get('user_id', 'unknown')}")
                              
                              # è·å–è¯¥ç”¨æˆ·çš„ç¬”è®°åˆ—è¡¨
                              note_list = await service.process_user_note_list(user_url, cookies)
                              logger.info(f"ğŸ“ è·å–åˆ° {len(note_list)} æ¡ç¬”è®°")
                              
                          except Exception as e:
                              logger.error(f"âŒ å¤„ç†ç”¨æˆ·å¤±è´¥: {user_url}, é”™è¯¯: {e}")
                              continue
                  
                  # 5. å¤„ç†ç¬”è®° URL
                  if note_urls_str:
                      note_urls = [url.strip() for url in note_urls_str.split(",") if url.strip()]
                      logger.info(f"ğŸ“‹ å‡†å¤‡çˆ¬å– {len(note_urls)} æ¡ç¬”è®°")
                      
                      for idx, note_url in enumerate(note_urls, 1):
                          try:
                              logger.info(f"[{idx}/{len(note_urls)}] æ­£åœ¨å¤„ç†ç¬”è®°: {note_url}")
                              note_data = await service.process_note_data(note_url, cookies)
                              logger.success(f"âœ… ç¬”è®°æ•°æ®å·²ä¿å­˜")
                              
                          except Exception as e:
                              logger.error(f"âŒ å¤„ç†ç¬”è®°å¤±è´¥: {note_url}, é”™è¯¯: {e}")
                              continue
                  
                  logger.success("ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼")
                  
              except Exception as e:
                  logger.error(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                  sys.exit(1)
              
              finally:
                  # 6. å…³é—­æ•°æ®åº“è¿æ¥
                  await close_mongo_connection()
                  # 7. å…³é—­ Selenium
                  if hasattr(service, 'api') and service.api:
                      service.api.close()
          
          if __name__ == "__main__":
              asyncio.run(main())
          EOF
          
          echo "âœ… æ‰§è¡Œè„šæœ¬å·²åˆ›å»º"
      
      # 9. æ‰§è¡Œçˆ¬è™«
      - name: ğŸ•·ï¸ æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
        working-directory: ./spider-api
        run: |
          python run_spider.py
          echo "çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå®Œæˆ"
      
      # 10. ä¸Šä¼ æ—¥å¿—ï¼ˆå¦‚æœå¤±è´¥ï¼‰
      - name: ğŸ“¤ ä¸Šä¼ æ—¥å¿—
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: spider-logs
          path: spider-api/spider_cron.log
          retention-days: 7
      
      # 11. å‘é€é€šçŸ¥ï¼ˆå¯é€‰ï¼‰
      - name: ğŸ“§ å‘é€æ‰§è¡Œç»“æœé€šçŸ¥
        if: always()
        run: |
          if [ ${{ job.status }} == 'success' ]; then
            echo "âœ… çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ"
          else
            echo "âŒ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥"
          fi
