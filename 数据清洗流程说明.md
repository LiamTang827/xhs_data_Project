# 数据清洗流程说明文档

> 📅 更新时间: 2026-02-06  
> 🎯 目标: 基于TikHub API数据生成创作者网络图

---

## 🔍 问题诊断

### 发现的问题

1. **TikHub API数据限制**
   - ❌ 返回的笔记数据**没有 `tag_list` 字段**
   - ❌ `user.fans` 经常为 `null`
   - ❌ `user.desc`, `user.ip_location` 也可能为 `null`
   - ✅ 但有完整的笔记互动数据：`likes`, `collected_count`, `comments_count`, `share_count`

2. **数据不一致**
   - `user_profiles`: 10个用户（但其中7个user_id为空或不匹配）
   - `user_snapshots`: 9个用户（有完整笔记数据）
   - **只有1个用户（图灵星球）同时在两个collection中**

3. **creator_networks 中的问题**
   - 很多创作者的 `followers: 0`, `engagementIndex: 0`
   - `topics`（流量密码）为空或不准确
   - 原因：之前的逻辑依赖profile，但profile数据不完整

---

## ✅ 解决方案

### 方案A：基于现有数据（无需API，已实现）

**脚本**: `backend/scripts/regenerate_creator_networks.py`

**数据来源**:
- 从 `user_snapshots` 读取9个用户的笔记数据
- 从笔记中提取用户信息和互动数据

**处理逻辑**:
1. **粉丝数**: 尝试从 `notes[0].user.fans` 获取，如果为null则设为0
2. **互动指数**: 从所有笔记计算
   ```python
   engagement_index = (
       total_likes + 
       total_collects * 2 + 
       total_comments * 3 + 
       total_shares * 5
   )
   ```
3. **话题标签 (topics)**:
   - 优先从 `user_profiles.profile_data.content_topics` 获取
   - 如果没有profile，从笔记标题简单分词提取高频关键词
   - 停用词过滤：排除"话题"、"小红书"、"视频"等无意义词

**运行方式**:
```bash
cd backend
python scripts/regenerate_creator_networks.py
```

**结果**:
- ✅ 9个创作者节点，全部有真实的互动指数
- ✅ 每个创作者都有话题标签（从profile或笔记提取）
- ⚠️ 粉丝数仍为0（TikHub API限制）
- ⚠️ 简单分词提取的topics质量一般（如"星球研究所"、"网络热传生物"）

---

### 方案B：AI分析生成高质量profile（需DeepSeek API）

**脚本**: `backend/scripts/process_all_snapshots.py`

**功能**:
1. 遍历所有 `user_snapshots` 中的用户
2. 调用 DeepSeek API 分析前20条笔记
3. 生成：
   - `content_topics`: 5-8个精准的话题标签（例：AI、编程、科普、财经）
   - `content_style`: 内容风格关键词
   - `user_style_embedding`: 512维向量（使用本地BAAI/bge-small-zh-v1.5模型）
4. 保存到 `user_profiles` 和 `user_embeddings`

**修改的prompt** (`collectors/xiaohongshu/analyzer.py`):
```python
system = (
    "你是一个小红书内容分析专家。请从用户的笔记中提取：\n"
    "1. **用户画像(user_style)**：包含persona、tone、interests\n"
    "2. **内容主题(content_topic)**：5-8个最核心的话题关键词\n\n"
    "⚠️ 注意：笔记中没有现成的标签，需要你从标题和描述中分析提取主题。\n"
    "💡 提取话题时优先选择：领域词、高频词、行业术语、品类词、场景词。\n"
    "例如：AI、编程、旅行、美食、科普、职场、Python、机器学习等。"
)
```

**⚠️ 注意**:
- 需要充值 DeepSeek API 额度
- 9个用户大约需要调用9次API（每次分析20条笔记）
- 估算费用：约 ¥0.5-1元（取决于笔记长度）

**运行方式**:
```bash
cd backend
python scripts/process_all_snapshots.py
```

---

## 📊 当前数据统计

### user_snapshots (9个用户)

| 昵称 | user_id | 笔记数 | 互动指数 | 当前topics来源 |
|------|---------|--------|----------|--------------|
| Unknown | 5ff98b9d... | 20 | 453,030 | 简单分词 |
| Unknown | 5ef2ec93... | 19 | 364,095 | 简单分词 |
| Unknown | 5abf9024... | 17 | 345,298 | 简单分词 |
| Unknown | 5e818a5d... | 20 | 473,938 | 简单分词 |
| Unknown | 586f4425... | 59 | 14,572 | 简单分词 |
| Unknown | 66d6aedc... | 20 | 112,245 | 简单分词 |
| Unknown | 5b218479... | 16 | 110 | 简单分词 |
| Unknown | 57576ed2... | 118 | 38,172 | 简单分词 |
| 图灵星球 | 5e647294... | 18 | 218 | ✅ profile |

### user_profiles (10个用户，但只有3个匹配)

| 昵称 | user_id | 是否有snapshots | topics质量 |
|------|---------|----------------|-----------|
| 星球研究所 | 空 | ❌ | AI生成 ✅ |
| 硅谷樱花小姐姐 | 空 | ❌ | AI生成 ✅ |
| 无穷小亮的科普日常 | 空 | ❌ | AI生成 ✅ |
| 小熊说你超有爱 | 空 | ❌ | AI生成 ✅ |
| 小Lin说 | 空 | ❌ | AI生成 ✅ |
| 大圆镜科普 | 空 | ❌ | AI生成 ✅ |
| Ada在美国 | 空 | ❌ | AI生成 ✅ |
| 所长林超 | 空 | ❌ | AI生成 ✅ |
| 图灵星球 | 5e647294... | ✅ | AI生成 ✅ |
| doobydobap | doobydobap | ❌ | AI生成 ✅ |

---

## 🛠️ 可用脚本说明

### 1. `regenerate_creator_networks.py` ⭐
**用途**: 基于snapshots重新生成创作者网络  
**需要API**: ❌ 不需要  
**输入**: user_snapshots collection  
**输出**: creator_networks collection  
**运行**:
```bash
python backend/scripts/regenerate_creator_networks.py
```

### 2. `process_all_snapshots.py`
**用途**: 为所有snapshots生成AI分析的profile  
**需要API**: ✅ DeepSeek API  
**输入**: user_snapshots collection  
**输出**: user_profiles + user_embeddings  
**运行**:
```bash
# 需要先充值DeepSeek API
python backend/scripts/process_all_snapshots.py
```

### 3. `analyze_database.py`
**用途**: 查看数据库现状  
**需要API**: ❌ 不需要  
**功能**: 展示所有用户的笔记数、互动数等统计信息  
**运行**:
```bash
python backend/scripts/analyze_database.py
```

### 4. `check_data_consistency.py`
**用途**: 检查profiles和snapshots的一致性  
**需要API**: ❌ 不需要  
**功能**: 找出哪些用户只在一个collection中存在  
**运行**:
```bash
python backend/scripts/check_data_consistency.py
```

### 5. `collectors/xiaohongshu/collector.py`
**用途**: 爬取新用户的笔记数据  
**需要API**: ✅ TikHub API  
**功能**: 获取用户的所有笔记并保存到snapshots  
**运行**:
```bash
cd collectors/xiaohongshu
# 修改USER_ID变量
python collector.py
```

---

## 📝 推荐工作流程

### 当前状态（无API额度）

1. ✅ 使用 `regenerate_creator_networks.py` 生成网络图
   - 基于现有的9个snapshots
   - topics质量一般但可用

2. ✅ 数据已经可以在前端展示
   - 互动指数是真实的
   - 有话题标签（虽然质量一般）

### 充值后的完整流程

1. 运行 `process_all_snapshots.py`
   - 为8个Unknown用户生成高质量profile
   - 生成准确的topics

2. 再次运行 `regenerate_creator_networks.py`
   - 此时所有用户都有AI生成的topics
   - 网络图会更准确

3. （可选）爬取更多创作者
   - 使用 `collector.py` 爬取新用户
   - 运行 `process_all_snapshots.py` 分析
   - 运行 `regenerate_creator_networks.py` 更新网络

---

## 🎯 数据质量对比

### 简单分词提取的topics（当前）
```
✅ 优点：不需要API，完全免费
❌ 缺点：
  - 质量一般：如"星球研究所原"、"网络热传生物"
  - 可能包含无意义词组
  - 不够精准和专业
```

### AI分析提取的topics（充值后）
```
✅ 优点：
  - 高质量：如"AI"、"编程"、"科普"、"财经"
  - 领域词准确
  - 适合做"流量密码"展示
❌ 缺点：需要API费用（约¥0.5-1元/9个用户）
```

---

## 💰 费用估算

- **DeepSeek API**: 
  - 输入：约15,000 tokens/用户 ≈ ¥0.003/用户
  - 输出：约200 tokens/用户 ≈ ¥0.0006/用户
  - **总计**: 9个用户 ≈ ¥0.03-0.05

- **TikHub API**:
  - 按次计费（具体价格见TikHub官网）
  - 爬取1个新用户约需要3-6次请求

---

## 📌 注意事项

1. **TikHub API限制**
   - fans字段经常为null
   - 没有tag_list字段
   - 一次返回约19-20条笔记

2. **数据一致性**
   - profiles和snapshots的user_id要保持一致
   - 当前有不匹配问题（7个profile的user_id为空）

3. **更新频率**
   - snapshots: 建议每周更新一次
   - profiles: 只需在添加新用户时生成一次
   - creator_networks: 可以随时重新生成（无成本）

---

**🎉 总结**: 现在你的数据流程已经完善，可以基于现有数据工作，也为未来的AI分析预留了接口。充值后直接运行 `process_all_snapshots.py` 即可获得高质量的创作者画像。
