"""
LLM Gateway - ç»Ÿä¸€çš„LLMè°ƒç”¨ç½‘å…³
å®ç°ç¼“å­˜ã€å‹ç¼©ã€é™æµç­‰ä¼˜åŒ–
"""

import hashlib
import json
import re
import asyncio
import uuid
import traceback
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from openai import OpenAI

from core.config import settings
from database.connection import get_database


class LLMGateway:
    """LLMç½‘å…³ - ç¼“å­˜ + å‹ç¼© + é™æµ"""
    
    def __init__(self):
        try:
            # ç®€åŒ–åˆå§‹åŒ–ï¼Œé¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜
            self.client = OpenAI(
                api_key=settings.DEEPSEEK_API_KEY,
                base_url=settings.DEEPSEEK_BASE_URL,
                timeout=30.0  # æ·»åŠ è¶…æ—¶è®¾ç½®
            )
            self.rate_limiter = TokenBucketRateLimiter(
                capacity=100,   # æ¡¶å®¹é‡
                refill_rate=10  # æ¯ç§’è¡¥å……10ä¸ªtoken
            )
            self.db = get_database()
            print("âœ… LLM Gateway åˆå§‹åŒ–å®Œæˆï¼ˆç¼“å­˜ + é™æµå·²å¯ç”¨ï¼‰")
        except Exception as e:
            print(f"âš ï¸  LLM Gateway åˆå§‹åŒ–è­¦å‘Š: {e}")
            # å³ä½¿åˆå§‹åŒ–å¤±è´¥ä¹Ÿåˆ›å»ºå®¢æˆ·ç«¯ï¼ˆç”¨äºéAIåŠŸèƒ½ï¼‰
            self.client = None
            self.rate_limiter = None
            self.db = get_database()
    
    async def chat(
        self,
        prompt: str,
        model: str = "deepseek-chat",
        max_tokens: int = 1000,
        temperature: float = 0.7,
        use_cache: bool = True
    ) -> str:
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            use_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        
        # ğŸ” è°ƒç”¨è¿½è¸ªï¼ˆè¯Šæ–­å¼‚å¸¸è°ƒç”¨ï¼‰
        call_id = str(uuid.uuid4())[:8]
        print(f"\n{'='*60}")
        print(f"[LLM #{call_id}] ğŸ“ æ–°çš„APIè°ƒç”¨è¯·æ±‚")
        print(f"[LLM #{call_id}] ğŸ• æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"[LLM #{call_id}] ğŸ¤– Model: {model}")
        print(f"[LLM #{call_id}] ğŸ“ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦ (å‹ç¼©å‰)")
        print(f"[LLM #{call_id}] âš™ï¸  Max Tokens: {max_tokens}")
        print(f"[LLM #{call_id}] ğŸ”¥ Temperature: {temperature}")
        print(f"[LLM #{call_id}] ğŸ’¾ Cache: {'å¯ç”¨' if use_cache else 'ç¦ç”¨'}")
        
        # è·å–è°ƒç”¨æ ˆï¼ˆè¯Šæ–­æ¥æºï¼‰
        stack = traceback.extract_stack()
        caller_info = None
        for frame in reversed(stack[:-1]):  # è·³è¿‡å½“å‰å‡½æ•°
            if 'llm_gateway' not in frame.filename:
                caller_info = f"{frame.filename}:{frame.lineno} in {frame.name}"
                break
        if caller_info:
            print(f"[LLM #{call_id}] ğŸ“ è°ƒç”¨æ¥æº: {caller_info}")
        print(f"{'='*60}\n")
        
        # 1ï¸âƒ£ Promptå‹ç¼©
        compressed_prompt = self._compress_prompt(prompt)
        print(f"[LLM #{call_id}] ğŸ—œï¸  å‹ç¼©åé•¿åº¦: {len(compressed_prompt)} å­—ç¬¦ (èŠ‚çœ {len(prompt) - len(compressed_prompt)} å­—ç¬¦)")
        
        # 2ï¸âƒ£ ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(compressed_prompt, model, temperature)
        
        # 3ï¸âƒ£ æ£€æŸ¥ç¼“å­˜ï¼ˆMongoDBï¼‰
        if use_cache:
            cached_response = await self._get_from_cache(cache_key)
            if cached_response:
                print(f"[LLM #{call_id}] ğŸ’° âœ… ç¼“å­˜å‘½ä¸­ï¼èŠ‚çœäº†APIè°ƒç”¨")
                print(f"[LLM #{call_id}] ğŸ¯ è¿”å›ç¼“å­˜å†…å®¹é•¿åº¦: {len(cached_response)} å­—ç¬¦\n")
                return cached_response
            else:
                print(f"[LLM #{call_id}] ğŸ’° âŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦è°ƒç”¨API")
        
        # 4ï¸âƒ£ é¢‘ç‡é™åˆ¶
        print(f"[LLM #{call_id}] â±ï¸  ç­‰å¾…é™æµå™¨æ”¾è¡Œ...")
        await self.rate_limiter.acquire()
        print(f"[LLM #{call_id}] âœ… é™æµå™¨å·²æ”¾è¡Œ")
        
        # 5ï¸âƒ£ è°ƒç”¨API
        try:
            print(f"[LLM #{call_id}] ğŸš€ æ­£åœ¨è°ƒç”¨DeepSeek API...")
            api_start_time = datetime.now()
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": compressed_prompt}],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            api_duration = (datetime.now() - api_start_time).total_seconds()
            result = response.choices[0].message.content
            
            # æ‰“å°è¯¦ç»†çš„APIä½¿ç”¨ç»Ÿè®¡
            usage = response.usage
            print(f"[LLM #{call_id}] âœ… APIè°ƒç”¨æˆåŠŸï¼")
            print(f"[LLM #{call_id}] â±ï¸  è€—æ—¶: {api_duration:.2f}ç§’")
            print(f"[LLM #{call_id}] ğŸ“Š Tokenç»Ÿè®¡:")
            print(f"[LLM #{call_id}]    - è¾“å…¥: {usage.prompt_tokens:,} tokens")
            print(f"[LLM #{call_id}]    - è¾“å‡º: {usage.completion_tokens:,} tokens")
            print(f"[LLM #{call_id}]    - æ€»è®¡: {usage.total_tokens:,} tokens")
            print(f"[LLM #{call_id}] ğŸ’µ ä¼°ç®—æˆæœ¬ (DeepSeek):")
            input_cost = usage.prompt_tokens * 0.27 / 1_000_000
            output_cost = usage.completion_tokens * 1.1 / 1_000_000
            print(f"[LLM #{call_id}]    - è¾“å…¥: ${input_cost:.6f}")
            print(f"[LLM #{call_id}]    - è¾“å‡º: ${output_cost:.6f}")
            print(f"[LLM #{call_id}]    - æœ¬æ¬¡æ€»è®¡: ${input_cost + output_cost:.6f}")
            print(f"[LLM #{call_id}] ğŸ“ è¿”å›å†…å®¹é•¿åº¦: {len(result)} å­—ç¬¦\n")
            
            # 6ï¸âƒ£ å†™å…¥ç¼“å­˜ï¼ˆTTL=24å°æ—¶ï¼‰
            if use_cache:
                await self._save_to_cache(cache_key, result)
                print(f"[LLM #{call_id}] ğŸ’¾ å·²ä¿å­˜åˆ°ç¼“å­˜")
            
            # 7ï¸âƒ£ è®°å½•ç»Ÿè®¡
            await self._log_usage(model, response.usage, compressed_prompt, result, call_id)
            
            return result
            
        except Exception as e:
            print(f"[LLM #{call_id}] âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _compress_prompt(self, prompt: str) -> str:
        """Promptå‹ç¼©ä¼˜åŒ–"""
        # 1. å»é™¤å¤šä½™ç©ºç™½
        compressed = ' '.join(prompt.split())
        
        # 2. ç§»é™¤HTMLæ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        compressed = re.sub(r'<[^>]+>', '', compressed)
        
        # 3. æˆªæ–­è¿‡é•¿å†…å®¹ï¼ˆä¿ç•™é‡è¦éƒ¨åˆ†ï¼‰
        if len(compressed) > 8000:
            # ä¿ç•™å¼€å¤´2000å­—ç¬¦ + ç»“å°¾2000å­—ç¬¦
            compressed = compressed[:2000] + "\n...(ä¸­é—´çœç•¥)...\n" + compressed[-2000:]
        
        return compressed
    
    def _generate_cache_key(self, prompt: str, model: str, temperature: float) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰"""
        content = f"{model}:{temperature}:{prompt}"
        return f"llm_cache:{hashlib.sha256(content.encode()).hexdigest()}"
    
    async def _get_from_cache(self, cache_key: str) -> Optional[str]:
        """ä»MongoDBç¼“å­˜è¯»å–"""
        cache_doc = self.db.llm_cache.find_one({"key": cache_key})
        if cache_doc:
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
            if datetime.now() - cache_doc['created_at'] < timedelta(hours=24):
                return cache_doc['response']
        return None
    
    async def _save_to_cache(self, cache_key: str, response: str):
        """ä¿å­˜åˆ°MongoDBç¼“å­˜"""
        self.db.llm_cache.update_one(
            {"key": cache_key},
            {
                "$set": {
                    "response": response,
                    "created_at": datetime.now()
                }
            },
            upsert=True
        )
    
    async def _log_usage(self, model: str, usage: Any, prompt: str, response: str, call_id: str = "unknown"):
        """è®°å½•ä½¿ç”¨ç»Ÿè®¡ï¼ˆå¢å¼ºç‰ˆ - åŒ…å«è°ƒç”¨è¿½è¸ªï¼‰"""
        
        # è·å–è°ƒç”¨æ ˆä¿¡æ¯
        stack = traceback.extract_stack()
        caller_file = "unknown"
        caller_line = 0
        caller_function = "unknown"
        
        for frame in reversed(stack[:-2]):  # è·³è¿‡å½“å‰å’Œchatå‡½æ•°
            if 'llm_gateway' not in frame.filename:
                caller_file = frame.filename.split('/')[-1]  # åªä¿ç•™æ–‡ä»¶å
                caller_line = frame.lineno
                caller_function = frame.name
                break
        
        log_data = {
            "call_id": call_id,
            "model": model,
            "prompt_tokens": usage.prompt_tokens,
            "completion_tokens": usage.completion_tokens,
            "total_tokens": usage.total_tokens,
            "prompt_length": len(prompt),
            "response_length": len(response),
            "timestamp": datetime.now(),
            "caller_file": caller_file,
            "caller_line": caller_line,
            "caller_function": caller_function
        }
        
        self.db.llm_usage_logs.insert_one(log_data)
        # å·²åœ¨ä¸Šé¢æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œè¿™é‡Œä¸é‡å¤æ‰“å°


class TokenBucketRateLimiter:
    """ä»¤ç‰Œæ¡¶é™æµå™¨"""
    
    def __init__(self, capacity: int, refill_rate: float):
        self.capacity = capacity
        self.refill_rate = refill_rate
        self.tokens = capacity
        self.last_refill = datetime.now()
        self._lock = asyncio.Lock()
    
    async def acquire(self, tokens: int = 1):
        """è·å–ä»¤ç‰Œï¼ˆé˜»å¡ç›´åˆ°æœ‰å¯ç”¨ä»¤ç‰Œï¼‰"""
        async with self._lock:
            while True:
                self._refill()
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    return
                await asyncio.sleep(0.1)
    
    def _refill(self):
        """è¡¥å……ä»¤ç‰Œ"""
        now = datetime.now()
        elapsed = (now - self.last_refill).total_seconds()
        new_tokens = elapsed * self.refill_rate
        self.tokens = min(self.capacity, self.tokens + new_tokens)
        self.last_refill = now


# å…¨å±€LLM Gatewayå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_llm_gateway = None

def get_llm_gateway() -> LLMGateway:
    """è·å–LLM Gatewayå•ä¾‹"""
    global _llm_gateway
    if _llm_gateway is None:
        _llm_gateway = LLMGateway()
    return _llm_gateway
