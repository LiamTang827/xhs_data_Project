"""
LLM Gateway - ç»Ÿä¸€çš„LLMè°ƒç”¨ç½‘å…³
å®ç°ç¼“å­˜ã€å‹ç¼©ã€é™æµç­‰ä¼˜åŒ–
"""

import hashlib
import json
import re
import asyncio
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from openai import OpenAI

from core.config import settings
from database.connection import get_database


class LLMGateway:
    """LLMç½‘å…³ - ç¼“å­˜ + å‹ç¼© + é™æµ"""
    
    def __init__(self):
        try:
            # ç®€åŒ–åˆå§‹åŒ–ï¼Œé¿å…ç‰ˆæœ¬å…¼å®¹é—®é¢˜
            self.client = OpenAI(
                api_key=settings.DEEPSEEK_API_KEY,
                base_url=settings.DEEPSEEK_BASE_URL,
                timeout=30.0  # æ·»åŠ è¶…æ—¶è®¾ç½®
            )
            self.rate_limiter = TokenBucketRateLimiter(
                capacity=100,   # æ¡¶å®¹é‡
                refill_rate=10  # æ¯ç§’è¡¥å……10ä¸ªtoken
            )
            self.db = get_database()
            print("âœ… LLM Gateway åˆå§‹åŒ–å®Œæˆï¼ˆç¼“å­˜ + é™æµå·²å¯ç”¨ï¼‰")
        except Exception as e:
            print(f"âš ï¸  LLM Gateway åˆå§‹åŒ–è­¦å‘Š: {e}")
            # å³ä½¿åˆå§‹åŒ–å¤±è´¥ä¹Ÿåˆ›å»ºå®¢æˆ·ç«¯ï¼ˆç”¨äºéAIåŠŸèƒ½ï¼‰
            self.client = None
            self.rate_limiter = None
            self.db = get_database()
    
    async def chat(
        self,
        prompt: str,
        model: str = "deepseek-chat",
        max_tokens: int = 1000,
        temperature: float = 0.7,
        use_cache: bool = True
    ) -> str:
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£
        
        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            use_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        
        # 1ï¸âƒ£ Promptå‹ç¼©
        compressed_prompt = self._compress_prompt(prompt)
        
        # 2ï¸âƒ£ ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(compressed_prompt, model, temperature)
        
        # 3ï¸âƒ£ æ£€æŸ¥ç¼“å­˜ï¼ˆMongoDBï¼‰
        if use_cache:
            cached_response = await self._get_from_cache(cache_key)
            if cached_response:
                print(f"[LLM Gateway] ğŸ’° ç¼“å­˜å‘½ä¸­: {cache_key[:16]}... (èŠ‚çœAPIè°ƒç”¨)")
                return cached_response
        
        # 4ï¸âƒ£ é¢‘ç‡é™åˆ¶
        await self.rate_limiter.acquire()
        
        # 5ï¸âƒ£ è°ƒç”¨API
        try:
            print(f"[LLM Gateway] ğŸš€ è°ƒç”¨API: {model} (tokensâ‰¤{max_tokens})")
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": compressed_prompt}],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            result = response.choices[0].message.content
            
            # 6ï¸âƒ£ å†™å…¥ç¼“å­˜ï¼ˆTTL=24å°æ—¶ï¼‰
            if use_cache:
                await self._save_to_cache(cache_key, result)
            
            # 7ï¸âƒ£ è®°å½•ç»Ÿè®¡
            await self._log_usage(model, response.usage, compressed_prompt, result)
            
            return result
            
        except Exception as e:
            print(f"[LLM Gateway] âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _compress_prompt(self, prompt: str) -> str:
        """Promptå‹ç¼©ä¼˜åŒ–"""
        # 1. å»é™¤å¤šä½™ç©ºç™½
        compressed = ' '.join(prompt.split())
        
        # 2. ç§»é™¤HTMLæ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        compressed = re.sub(r'<[^>]+>', '', compressed)
        
        # 3. æˆªæ–­è¿‡é•¿å†…å®¹ï¼ˆä¿ç•™é‡è¦éƒ¨åˆ†ï¼‰
        if len(compressed) > 8000:
            # ä¿ç•™å¼€å¤´2000å­—ç¬¦ + ç»“å°¾2000å­—ç¬¦
            compressed = compressed[:2000] + "\n...(ä¸­é—´çœç•¥)...\n" + compressed[-2000:]
        
        return compressed
    
    def _generate_cache_key(self, prompt: str, model: str, temperature: float) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰"""
        content = f"{model}:{temperature}:{prompt}"
        return f"llm_cache:{hashlib.sha256(content.encode()).hexdigest()}"
    
    async def _get_from_cache(self, cache_key: str) -> Optional[str]:
        """ä»MongoDBç¼“å­˜è¯»å–"""
        cache_doc = self.db.llm_cache.find_one({"key": cache_key})
        if cache_doc:
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
            if datetime.now() - cache_doc['created_at'] < timedelta(hours=24):
                return cache_doc['response']
        return None
    
    async def _save_to_cache(self, cache_key: str, response: str):
        """ä¿å­˜åˆ°MongoDBç¼“å­˜"""
        self.db.llm_cache.update_one(
            {"key": cache_key},
            {
                "$set": {
                    "response": response,
                    "created_at": datetime.now()
                }
            },
            upsert=True
        )
    
    async def _log_usage(self, model: str, usage: Any, prompt: str, response: str):
        """è®°å½•ä½¿ç”¨ç»Ÿè®¡"""
        log_data = {
            "model": model,
            "prompt_tokens": usage.prompt_tokens,
            "completion_tokens": usage.completion_tokens,
            "total_tokens": usage.total_tokens,
            "prompt_length": len(prompt),
            "response_length": len(response),
            "timestamp": datetime.now()
        }
        
        self.db.llm_usage_logs.insert_one(log_data)
        print(f"[LLM Gateway] ğŸ“Š Tokenæ¶ˆè€—: {usage.total_tokens} (æç¤º:{usage.prompt_tokens} + å®Œæˆ:{usage.completion_tokens})")


class TokenBucketRateLimiter:
    """ä»¤ç‰Œæ¡¶é™æµå™¨"""
    
    def __init__(self, capacity: int, refill_rate: float):
        self.capacity = capacity
        self.refill_rate = refill_rate
        self.tokens = capacity
        self.last_refill = datetime.now()
        self._lock = asyncio.Lock()
    
    async def acquire(self, tokens: int = 1):
        """è·å–ä»¤ç‰Œï¼ˆé˜»å¡ç›´åˆ°æœ‰å¯ç”¨ä»¤ç‰Œï¼‰"""
        async with self._lock:
            while True:
                self._refill()
                if self.tokens >= tokens:
                    self.tokens -= tokens
                    return
                await asyncio.sleep(0.1)
    
    def _refill(self):
        """è¡¥å……ä»¤ç‰Œ"""
        now = datetime.now()
        elapsed = (now - self.last_refill).total_seconds()
        new_tokens = elapsed * self.refill_rate
        self.tokens = min(self.capacity, self.tokens + new_tokens)
        self.last_refill = now


# å…¨å±€LLM Gatewayå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_llm_gateway = None

def get_llm_gateway() -> LLMGateway:
    """è·å–LLM Gatewayå•ä¾‹"""
    global _llm_gateway
    if _llm_gateway is None:
        _llm_gateway = LLMGateway()
    return _llm_gateway
